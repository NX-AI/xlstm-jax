#!/bin/bash -l
#SBATCH --job-name=xlstm-jax
#SBATCH --nodes=1
#SBATCH --partition=compute
#SBATCH --gres=gpu:8
#SBATCH --gpu-bind=none
#SBATCH --cpus-per-task=12  # Number of cores per socket on compute nodes
#SBATCH --ntasks-per-node=8
#SBATCH --wait-all-nodes=1
#SBATCH --time=0-00:20:00
#SBATCH --chdir=/home/plippe/xlstm-jax
#SBATCH --output=/nfs-gpu/xlstm/logs/plippe_test/outputs/xlstm-jax/%j.out
#SBATCH --exclusive

#-------------------------------------------------------------------------------------------------
# Below is from oracle examples. I used the container example, as it uses srun instead of mpirun
export PMI_DEBUG=1

MACHINEFILE="/nfs-gpu/xlstm/logs/plippe_test/outputs/xlstm-jax/hostfile"

scontrol show hostnames $SLURM_JOB_NODELIST > $MACHINEFILE
echo MACHINEFILE
cat $MACHINEFILE

source /etc/os-release

# BELOW SNIPPET FROM https://github.com/oracle-quickstart/oci-hpc/blob/master/samples/gpu/nccl_run_allreduce_H100.sbatch
mpivars_path=`ls /usr/mpi/gcc/openmpi-*/bin/mpivars.sh`

if [[ "$mpivars_path" == "" ]]; then
    mpivars_path=`ls /opt/openmpi-*/bin/mpivars.sh`
fi

if [[ "$mpivars_path" == "" ]]; then
    echo "Could not find MPIPATH"; exit; fi

source $mpivars_path

# Had to remove a part from example sbatch that checks whether the node has H100 GPUs:
# https://github.com/oracle-quickstart/oci-hpc/blob/master/samples/gpu/nccl_run_allreduce_containers_H100.sbatch#L33-L39
# It is not necessary (we know we have H100), not useful (does not raise error and stop, just echo), and breaks our code (head node returns VM.Standard.E4.Flex, not BM.GPU.H100.8)
export NCCL_CROSS_NIC=0 \
       NCCL_SOCKET_NTHREADS=16 \
       NCCL_DEBUG=WARN \
       NCCL_CUMEM_ENABLE=0 \
       NCCL_IB_SPLIT_DATA_ON_QPS=0 \
       NCCL_IB_QPS_PER_CONNECTION=16 \
       NCCL_IB_GID_INDEX=3 \
       NCCL_IB_TC=41 \
       NCCL_IB_SL=0 \
       NCCL_IB_TIMEOUT=22 \
       NCCL_NET_PLUGIN=none \
       NCCL_SOCKET_IFNAME=eth0 \
       NCCL_IGNORE_CPU_AFFINITY=1 \
       NCCL_IB_HCA="=mlx5_0,mlx5_1,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_12,mlx5_13,mlx5_14,mlx5_15,mlx5_16,mlx5_17" \
       HCOLL_ENABLE_MCAST_ALL=0 \
       coll_hcoll_enable=0 \
       UCX_TLS=tcp \
       UCX_NET_DEVICES=eth0 \
       RX_QUEUE_LEN=8192 \
       IB_RX_QUEUE_LEN=8192 \
       OMPI_MCA_coll=^hcoll
    #    NCCL_TOPO_FILE=/nfs/cluster/H100-topology.xml \

# Needed for CUDA compatibility with older drivers on H100
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/plippe/.conda/envs/jax0431_local/cuda-compat/


# --------------------------------------------------------------------------------------------------
# Above env variables taken from oracle examples. Below comes our stuff.

# activate specified conda env
echo "Activating conda env" jax0431_plippe
conda activate jax0431_plippe

# set the first node name as master address master port is retrieved in the script
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
export MASTER_PORT=$(expr 12000 + $(echo -n $SLURM_JOBID | tail -c 4))


# run
srun --kill-on-bad-exit=1 --mpi=pmi2 --gpus-per-node=$SBATCH_GPUS_PER_NODE \
     --ntasks-per-node=$SLURM_NTASKS_PER_NODE \
     bash -c 'source $mpivars_path && Starting run; PYTHONPATH=. python scripts/run_benchmark.py --model="30B"'
