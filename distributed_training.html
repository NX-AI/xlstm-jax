
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Distributed Training &#8212; xlstm-jax  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=2eb6d7a9" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'distributed_training';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="autoapi/index.html" />
    <link rel="prev" title="Configuring Experiments with Hydra" href="configuration_with_hydra.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nxai_logo_light.svg" class="logo__image only-light" alt="xlstm-jax  documentation - Home"/>
    <img src="_static/nxai_logo_dark.svg" class="logo__image only-dark pst-js-only" alt="xlstm-jax  documentation - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_preparation.html">Dataset Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_training.html">Training large language models in xlstm-jax</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration_with_hydra.html">Configuring Experiments with Hydra</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Distributed Training</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autoapi/index.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="autoapi/xlstm_jax/index.html">xlstm_jax</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/common_types/index.html">xlstm_jax.common_types</a></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/configs/index.html">xlstm_jax.configs</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="autoapi/xlstm_jax/dataset/index.html">xlstm_jax.dataset</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/batch/index.html">xlstm_jax.dataset.batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/configs/index.html">xlstm_jax.dataset.configs</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/grain_batch_rampup/index.html">xlstm_jax.dataset.grain_batch_rampup</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/grain_data_processing/index.html">xlstm_jax.dataset.grain_data_processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/grain_iterator/index.html">xlstm_jax.dataset.grain_iterator</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/grain_transforms/index.html">xlstm_jax.dataset.grain_transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/hf_tokenizer/index.html">xlstm_jax.dataset.hf_tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/input_pipeline_interface/index.html">xlstm_jax.dataset.input_pipeline_interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/lmeval_dataset/index.html">xlstm_jax.dataset.lmeval_dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/lmeval_pipeline/index.html">xlstm_jax.dataset.lmeval_pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/multihost_dataloading/index.html">xlstm_jax.dataset.multihost_dataloading</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/dataset/synthetic_dataloading/index.html">xlstm_jax.dataset.synthetic_dataloading</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/define_hydra_schemas/index.html">xlstm_jax.define_hydra_schemas</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="autoapi/xlstm_jax/distributed/index.html">xlstm_jax.distributed</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/array_utils/index.html">xlstm_jax.distributed.array_utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/data_parallel/index.html">xlstm_jax.distributed.data_parallel</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/mesh_utils/index.html">xlstm_jax.distributed.mesh_utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/pipeline_parallel/index.html">xlstm_jax.distributed.pipeline_parallel</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/single_gpu/index.html">xlstm_jax.distributed.single_gpu</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/tensor_parallel/index.html">xlstm_jax.distributed.tensor_parallel</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/distributed/xla_utils/index.html">xlstm_jax.distributed.xla_utils</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/import_utils/index.html">xlstm_jax.import_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/main_train/index.html">xlstm_jax.main_train</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="autoapi/xlstm_jax/models/index.html">xlstm_jax.models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/models/configs/index.html">xlstm_jax.models.configs</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/models/llama/index.html">xlstm_jax.models.llama</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/models/shared/index.html">xlstm_jax.models.shared</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/models/xlstm_clean/index.html">xlstm_jax.models.xlstm_clean</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/models/xlstm_parallel/index.html">xlstm_jax.models.xlstm_parallel</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/models/xlstm_pytorch/index.html">xlstm_jax.models.xlstm_pytorch</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/resume_training/index.html">xlstm_jax.resume_training</a></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/start_training/index.html">xlstm_jax.start_training</a></li>
<li class="toctree-l3"><a class="reference internal" href="autoapi/xlstm_jax/train_init_fns/index.html">xlstm_jax.train_init_fns</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="autoapi/xlstm_jax/trainer/index.html">xlstm_jax.trainer</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/base/index.html">xlstm_jax.trainer.base</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/callbacks/index.html">xlstm_jax.trainer.callbacks</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/data_module/index.html">xlstm_jax.trainer.data_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/llm/index.html">xlstm_jax.trainer.llm</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/logger/index.html">xlstm_jax.trainer.logger</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/metrics/index.html">xlstm_jax.trainer.metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/trainer/optimizer/index.html">xlstm_jax.trainer.optimizer</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="autoapi/xlstm_jax/utils/index.html">xlstm_jax.utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/utils/error_logging_utils/index.html">xlstm_jax.utils.error_logging_utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/utils/model_param_handling/index.html">xlstm_jax.utils.model_param_handling</a></li>
<li class="toctree-l4"><a class="reference internal" href="autoapi/xlstm_jax/utils/pytree_utils/index.html">xlstm_jax.utils.pytree_utils</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/lmeval_extended_evaluation/index.html">lmeval_extended_evaluation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/distributed_training.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Distributed Training</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-computing-in-jax">Distributed Computing in JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jit-vs-shard-map">JIT vs Shard Map</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-host-training">Multi-Host Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization-strategies">Parallelization Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-parallelism">Data Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-sharded-data-parallelism">Fully-Sharded Data Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-parallelism">Pipeline Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallelism">Tensor Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="distributed-training">
<h1>Distributed Training<a class="headerlink" href="#distributed-training" title="Link to this heading">#</a></h1>
<p>xLSTM-jax supports different parallelization strategies (data, fsdp, tensor) and activation checkpointing (referred to as remat in JAX),
enabling efficient training on large-scale distributed systems with hundreds or thousands of GPUs.
For an in-depth tutorial on 3D parallelization in JAX, please see <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/overview.html">Training Models At Scale</a> by Phillip Lippe,
covering</p>
<ul class="simple">
<li><p><a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/data_parallel_fsdp.html#Data-Parallelism">Data Parallelism (DP)</a></p></li>
<li><p><a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/data_parallel_fsdp.html#Parameter-Sharding">Fully-sharded Data Parallelism (FSDP)</a></p></li>
<li><p><a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/tensor_parallel_simple.html">Tensor Parallelism (TP)</a></p></li>
</ul>
<p>Several default parallelization configurations are located in <code class="docutils literal notranslate"><span class="pre">configs/parallel</span></code> directory.
For model sizes of up to 7B parameters, trained on up to 256 H100 GPUs, we found the combination of FSDP and activation checkpointing (remat) to be the most performant on our cluster (different hardware setups may favor different setups).
Tensor parallelization is required for larger models.</p>
<section id="distributed-computing-in-jax">
<h2>Distributed Computing in JAX<a class="headerlink" href="#distributed-computing-in-jax" title="Link to this heading">#</a></h2>
<section id="jit-vs-shard-map">
<h3>JIT vs Shard Map<a class="headerlink" href="#jit-vs-shard-map" title="Link to this heading">#</a></h3>
<p>In JAX, distributed computing can be implemented by way of automatic parallelization with <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html"><code class="docutils literal notranslate"><span class="pre">jax.jit</span></code></a> or manual sharding with <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/shard_map.html"><code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a>. In our initial experiments, we obtained significantly better performance in reducing communication bottlenecks with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code> than with <code class="docutils literal notranslate"><span class="pre">jit</span></code> on our compute cluster. Hence, we use <code class="docutils literal notranslate"><span class="pre">shard_map</span></code> for our parallelization strategies, but note that for other hardware setups, <code class="docutils literal notranslate"><span class="pre">jit</span></code> may be more performant.</p>
</section>
<section id="multi-host-training">
<h3>Multi-Host Training<a class="headerlink" href="#multi-host-training" title="Link to this heading">#</a></h3>
<p>For multi-host training, one JAX process needs to be started per host (see <a class="reference external" href="https://jax.readthedocs.io/en/latest/multi_process.html">official documentation</a>). On a SLURM cluster, we start by default one process per GPU by way of <code class="docutils literal notranslate"><span class="pre">jax.distributed.initialize()</span></code>, which is coordinated by the environment variables set by SLURM. On a single host, we also support running a single process for all GPUs (for example for quick debugging).</p>
<p>Each JAX process starts its own data loading pipeline, in which each process loads only its respective shard of the full dataset. The batch size is then divided by the number of processes. For model parallelization strategies (TP or PP), we first gather the batch over the model parallel dimension before applying the model forward pass. To support batch sizes smaller than 1 per device, one could integrate “fake” data loaders like in <a class="reference external" href="https://github.com/AI-Hypercomputer/maxtext">MaxText</a> and slice the batch accordingly within the forward pass. However, as this is not needed for a 7B model scale on H100s, we are not yet supporting this.</p>
</section>
</section>
<section id="parallelization-strategies">
<h2>Parallelization Strategies<a class="headerlink" href="#parallelization-strategies" title="Link to this heading">#</a></h2>
<p>For a detailed explanation of the parallelization strategies with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code>, please see the <a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/overview.html">Training Models At Scale</a> overview. We create a distributed 4D mesh with the following dimensions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dp</span></code> (data parallel)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fsdp</span></code> (fully sharded data parallel)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pp</span></code> (pipeline parallel)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tp</span></code> (tensor parallel)</p></li>
</ul>
<p>For our 7B model training on 256 H100 GPUs, we used a mesh shape of <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">1)</span></code> (ie FSDP within a node, DP across nodes).</p>
<section id="data-parallelism">
<h3>Data Parallelism<a class="headerlink" href="#data-parallelism" title="Link to this heading">#</a></h3>
<p>In data parallelism, the model is replicated across all devices, and each device processes a different batch of data. The gradients are then averaged across all devices. While some gradient communications can be performed asynchronously, note that stacking parameters by way of the scan transformation over layers may force some communications to only happen after all layers have been processed. However, this did not significantly impact our training performance.</p>
</section>
<section id="fully-sharded-data-parallelism">
<h3>Fully-Sharded Data Parallelism<a class="headerlink" href="#fully-sharded-data-parallelism" title="Link to this heading">#</a></h3>
<p>Fully-sharded data parallelism (FSDP) extends DP by sharding the model parameters and optimizer state across all devices. Before the forward pass of a module, we gather the model parameters over the <code class="docutils literal notranslate"><span class="pre">fsdp</span></code> dimension. Similarly, during the backward pass, we scatter the gradients over the <code class="docutils literal notranslate"><span class="pre">fsdp</span></code> dimension. This strategy reduces the memory footprint of the model on each device, as only a fraction of the model is stored on each device.</p>
<p>Separating the DP and FSDP dimensions allows for a flexible combination of both strategies. For example, we can use FSDP within a node to take advantage of its fast communication, and DP across nodes to scale to a large number of devices. Note that most FSDP communications can be performed asynchronously, which can further reduce the communication overhead. To prevent minor tensors from being sharded (for example biases or input/forget gate), we only shard tensors with a size larger than a certain threshold (specified in config). Additionally, for faster communication, we support gather the weights in different precisions (for example <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>), if they are casted to the same precision before the forward pass.</p>
</section>
<section id="pipeline-parallelism">
<h3>Pipeline Parallelism<a class="headerlink" href="#pipeline-parallelism" title="Link to this heading">#</a></h3>
<p>Pipeline parallelism (PP) splits the model “vertically” (that is layer dimension) across the <code class="docutils literal notranslate"><span class="pre">pp</span></code> dimension. However, due to the max training size of 256 GPUs, we are not actively using and supporting PP in our current setup. For model parallelism, we recommend using TP instead.</p>
</section>
<section id="tensor-parallelism">
<h3>Tensor Parallelism<a class="headerlink" href="#tensor-parallelism" title="Link to this heading">#</a></h3>
<p>Tensor parallelism (TP) splits the model “horizontally” (that is feature dimension) across the <code class="docutils literal notranslate"><span class="pre">tp</span></code> dimension, which is useful for models that do not fit on a single device. As the xLSTM-7B model has a very similar architecture to a standard Transformer, we can apply the same tensor parallelization strategies: in the mLSTM block, we split the heads across the <code class="docutils literal notranslate"><span class="pre">tp</span></code> dimension, and in the feedforward block, we split the up-projected hidden dimension across the <code class="docutils literal notranslate"><span class="pre">tp</span></code> dimension. As TP can introduce some communication bottlenecks, we support the asynchronous TP execution of linear layers, similar to the <a class="reference external" href="https://arxiv.org/abs/2302.05442">ViT-22b</a>, but note that GPUs with fully connected NVLinks may not benefit from this due to the communication layout.</p>
</section>
<section id="miscellaneous">
<h3>Miscellaneous<a class="headerlink" href="#miscellaneous" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Activation Checkpointing (Remat)</strong>: We use activation checkpointing (remat) to reduce the memory footprint of the model and to reduce the communication overhead. This is especially important for large models like xLSTM-7B, as it allows us to trade off memory for computation. We use the <code class="docutils literal notranslate"><span class="pre">remat</span></code> transformation around the individual blocks of the xLSTM model.</p></li>
<li><p><strong>Gradient Accumulation</strong>: We support gradient accumulation across multiple steps, which can be useful for large batch sizes or for models that do not fit on a single device.</p></li>
<li><p><strong>Mixed Precision Training</strong>: We support mixed precision training with setting the JAX dtype in the model config (for example to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>), which can reduce the memory footprint and increase the training speed.</p></li>
<li><p><strong>Distributed Gradient Clipping</strong>: We support gradient clipping to prevent exploding gradients, which computes the norm across all (distributed) gradients and clips them accordingly.</p></li>
</ul>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="configuration_with_hydra.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Configuring Experiments with Hydra</p>
      </div>
    </a>
    <a class="right-next"
       href="autoapi/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-computing-in-jax">Distributed Computing in JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jit-vs-shard-map">JIT vs Shard Map</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-host-training">Multi-Host Training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization-strategies">Parallelization Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-parallelism">Data Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-sharded-data-parallelism">Fully-Sharded Data Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-parallelism">Pipeline Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallelism">Tensor Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By NXAI GmbH
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, NXAI GmbH.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>