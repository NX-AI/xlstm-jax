defaults:
  - parallel_schema
  - _self_

# Size of the data axis. If -1, it will be inferred by the number of available devices.
data_axis_size: -1

# Size of the FSDP axis. If -1, it will be inferred by the number of available devices.
fsdp_axis_size: 1

# Size of the pipeline axis. If -1, it will be inferred by the number of available devices.
pipeline_axis_size: 1

# Size of the model axis. If -1, it will be inferred by the number of available devices.
model_axis_size: 1

# Name of the data axis.
data_axis_name: dp

# Name of the FSDP axis.
fsdp_axis_name: fsdp

# Name of the pipeline axis.
pipeline_axis_name: pipe

# Name of the model axis.
model_axis_name: tp

# Module names on which we apply activation checkpointing / rematerialization.
remat: []

# Module names on which we apply FSDP sharding.
fsdp_modules: []

# Minimum size of a parameter to be sharded with FSDP.
fsdp_min_weight_size: 262144  # = 2**18

# The dtype to cast the parameters to before gathering with FSDP.
# If `None`, no casting is performed and parameters are gathered in original precision (e.g. `float32`).
fsdp_gather_dtype: ~

# The dtype to cast the gradients to before scattering.
# If `None`, the dtype of the parameters is used.
fsdp_grad_scatter_dtype: ~

# Whether to use asynchronous tensor parallelism for dense layers.
# Default to `False`, as on local hardware,
# ppermute communication introduces large overhead.
tp_async_dense: false
