defaults:
  - model_schema
  - _self_

# LlamaConfig
vocab_size: 50304
add_qk_norm: false
theta: 10_000
scan_blocks: true
add_embedding_dropout: false
dtype: bfloat16
attention_backend: cudnn
