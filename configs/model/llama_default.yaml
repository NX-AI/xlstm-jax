defaults:
  - model_schema
  - _self_

# LlamaConfig
vocab_size: 50304
add_qk_norm: false
scan_blocks: true
add_embedding_dropout: false
dtype: bfloat16
use_flash_attention: true
