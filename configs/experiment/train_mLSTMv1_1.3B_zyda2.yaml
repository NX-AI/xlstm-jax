# @package _global_
defaults:
  - /data@data_train.val1: fineweb_edu_arrayrecord_train
  - /data@data_train.val2: dclm_arrayrecord_train
  - /data@data_train.val3: zyda2_zyda_arrayrecord_train
  - /data@data_train.val4: zyda2_dolmacc_arrayrecord_train
  - /data@data_eval.val1: slimpajama_627B_arrayrecord_eval_preprocessed
  - /data@data_eval.val2: dclm_arrayrecord_eval_preprocessed
  - override /parallel: mLSTMv1_1.3B
  - override /model: mLSTMv1_1.3B
  - override /optimizer: adamw
  - override /data: dclm_arrayrecord_train
  - _self_

# specify the deltas from the defaults:
task_name: Zyda
batch_size_per_device: 8
context_length: 8192
num_epochs: 1000
num_train_steps: 95_000
lr: 7e-4

trainer:
  gradient_accumulate_steps: 1
  check_val_every_n_steps: 5_000
  log_logit_stats: false
  log_intermediates: false

checkpointing:
  monitor: dclm_perplexity

# The following is not directly used in the experiment, but for logging purposes.
data:
  # Name of the dataset. Used for logging.
  name: zyda
  tokenizer_path: "EleutherAI/gpt-neox-20b"

data_train:
  val1:
    tokenizer_path: "EleutherAI/gpt-neox-20b"
  weight1: 4.0
  val2:
    tokenizer_path: "EleutherAI/gpt-neox-20b"
  weight2: 4.0
  val3:
    tokenizer_path: "EleutherAI/gpt-neox-20b"
  weight3: 0.16
  val4:
    tokenizer_path: "EleutherAI/gpt-neox-20b"
  weight4: 0.24

data_eval:
  val1:
    tokenizer_path: "EleutherAI/gpt-neox-20b"
  val2:
    tokenizer_path: "EleutherAI/gpt-neox-20b"

model:
  reset_at_document_boundaries: true
  vocab_size: -1
