# @package _global_
defaults:
  - /data@data_train.ds1: dclm_arrayrecord_train
  - /data@data_eval.ds1: slimpajama_627B_arrayrecord_eval_preprocessed
  - /data@data_eval.ds2: dclm_arrayrecord_eval_preprocessed
  - override /parallel: llama7B
  - override /model: llama7B
  - override /optimizer: adamw
  - override /scheduler: exponential_decay
  - override /hydra/launcher: slurm_launcher
  - _self_

# specify the deltas from the defaults:
task_name: DCLM
batch_size_per_device: 8
context_length: 8192
num_epochs: 1000
num_train_steps: 95_000
lr: 5e-4

scheduler:
  warmup_steps: 3000

trainer:
  gradient_accumulate_steps: 1
  check_val_every_n_steps: 5_000
  log_logit_stats: false
  log_intermediates: false

checkpointing:
  monitor: dclm_perplexity

data_train:
  ds1:
    tokenizer_path: "EleutherAI/gpt-neox-20b"

data_eval:
  ds1:
    tokenizer_path: "EleutherAI/gpt-neox-20b"
  ds2:
    tokenizer_path: "EleutherAI/gpt-neox-20b"

model:
  reset_at_document_boundaries: true
  vocab_size: -1
  theta: 500_000

# Run command:
# PYTHONPATH=. python scripts/training/train_with_hydra.py +experiment=train_llama7B_dclm.yaml
hydra:
  launcher:
    nodes: 16
    additional_parameters: {
      "gpu-bind": "closest",
      "wait-all-nodes": "1",
      "time": "21-00:00:00",
      "exclusive": "",
    }
  mode: MULTIRUN
