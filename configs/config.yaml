# @package _global_
defaults:
  - config_schema
  - parallel: synthetic
  - model: mLSTM120M
  - scheduler: exponential_decay
  - optimizer: adamw
  - checkpointing: model_checkpointing
  - lr_monitor: lr_monitor_config
  - profiling: jax_profiling
  - logger: default_logger
  - trainer: default_llm_trainer
  - _self_


# General hyperparameters. Will be put in their respective config modules once they are created.

# Device. cpu or gpu
device: gpu
device_count: 1   # note: this is only used when using device=cpu and specifies the number of devices
                  #       that jax simulates on a CPU. Useful for multiprocess debugging.
n_gpus: 8
# TODO: having device_count and n_gpus is confusing. We have to decide on a naming scheme that encompasses
#       both jax-simulated CPU and GPU devices.

# Training-related
batch_size_per_device: 8  # will be multiplied by the number of devices used to obtain the global batch size
global_batch_size: ???
lr: 1e-3
context_length: 2048
num_epochs: 1000
#num_train_steps: 95_000

# Task name determines output directory
task_name: ??? # must be set in the experiment file
logging_name: ${data_train.ds1.name}_${model.name}_ctx${context_length}

# Set hydra working dir
base_dir: /nfs-gpu/xlstm/logs/outputs/xlstm-jax # this should be changed when running local experiments
hydra:
  run:
    dir: ${base_dir}/${task_name}/${logging_name}_${now:%Y-%m-%dT%H:%M:%S}
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}
