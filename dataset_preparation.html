

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Dataset Preparation &mdash; xlstm-jax  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            xlstm-jax
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="configuration_with_hydra.html">Configuring Experiments with Hydra</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoapi/index.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">xlstm-jax</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Dataset Preparation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/dataset_preparation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="dataset-preparation">
<h1>Dataset Preparation<a class="headerlink" href="#dataset-preparation" title="Link to this heading"></a></h1>
<p>The data preprocessing pipeline and multi-host iterator is based on <a class="reference external" href="https://github.com/google/grain">Google Grain</a>, using the <a class="reference external" href="https://github.com/google/array_record">arrayrecord</a> dataset format.
Here we explain how to convert huggingface datasets to arrayrecord for our data preprocessing pipeline.</p>
<section id="convert-huggingface-datasets-to-arrayrecord">
<h2>Convert Huggingface Datasets to ArrayRecord<a class="headerlink" href="#convert-huggingface-datasets-to-arrayrecord" title="Link to this heading"></a></h2>
<p>Downloading and converting to arrayrecords is done by way of the script hf_to_arrayrecord.py.
Example: <code class="docutils literal notranslate"><span class="pre">PYTHONPATH=.</span> <span class="pre">python</span> <span class="pre">scripts/data_processing/hf_to_arrayrecord.py</span> <span class="pre">--hf_path=DKYoon/SlimPajama-6B</span></code>.
For standard pre-training datasets (for example DCLM, Slimpajama, Zyda-v2), only the text column is read and converted.
For Question/Answer style instruction datasets, used for the cooldown phase in pretraining, the different messages (with roles ‘system’, ‘user’ and ‘assistant’) will simply be concatenated.
These instruction datasets all have different column namings, new datasets must be added manually to the script.</p>
</section>
<section id="splitting-dclm-dataset">
<h2>Splitting DCLM dataset<a class="headerlink" href="#splitting-dclm-dataset" title="Link to this heading"></a></h2>
<p>The DCLM dataset was released with a single (train) split. We created our own validation split with 500k sequences using the script <code class="docutils literal notranslate"><span class="pre">scripts/split_array_records_dataset.py</span></code>.
The script reads the DCLM arrayrecords dataset and produces a training and validation split in arrayrecords.
<code class="docutils literal notranslate"><span class="pre">PYTHONPATH=.</span> <span class="pre">python</span> <span class="pre">scripts/data_processing/split_array_records_dataset.py</span> <span class="pre">--dataset_name=DCLM</span></code></p>
</section>
<section id="preprocess-validation-datasets">
<h2>Preprocess Validation Datasets<a class="headerlink" href="#preprocess-validation-datasets" title="Link to this heading"></a></h2>
<p>We preprocessed the validation dataset of DCLM and SlimPajama627B using the script <code class="docutils literal notranslate"><span class="pre">scripts/preprocess_ar_dataset.py</span></code>.
This script uses our data preprocessing pipeline to preprocess the text data into tokenized and packed sequences.
The script currently supports only DCLM, SlimPajama627B and SlimPajama6B, but can be extended.</p>
</section>
<section id="dclm-dataset">
<h2>DCLM dataset<a class="headerlink" href="#dclm-dataset" title="Link to this heading"></a></h2>
<p>The steps to create the DCLM dataset are as follows:</p>
<ul class="simple">
<li><p>download the <a class="reference external" href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0-parquet">DCLM-parquet version</a> from huggingface, since the standard version has bugs and cannot be loaded.</p></li>
<li><p>convert to ArrayRecord.</p></li>
<li><p>split dataset into train and validation.</p></li>
<li><p>preprocess the validation set.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, NXAI.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>